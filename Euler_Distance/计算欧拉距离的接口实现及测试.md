# 计算欧拉距离的接口实现及测试

> 通过阅读论文，得知优化计算欧拉距离的基本并行方式有两种，一个是直接写一个kernel对该计算过程进行并行优化，另一个是将计算欧拉距离化为经典问题“矩阵乘矩阵”的变形，可以基于已有的库进行优化。分别对以上两种算法进行实现和测试。
>
> 定义矩阵A，dim\*m，矩阵B，dim\*n，输出距离矩阵C为，m\*n。这样定义矩阵有利于并行时使用memory coalescing技术。

## 使用kernel直接优化

### 编写kernel思路

串行计算欧拉距离的算法流程为：

* 取矩阵A中的一个点向量和矩阵B中的一个点向量，对应相减求平方和后的开方。
* 循环上一步直到穷尽取完矩阵A中所有点向量和矩阵B中所有点向量的组合。

该算法的复杂度为O(m \* n \* dim)。

针对点向量和点向量之间的计算没有依赖性这一特点，我先后尝试了以下几个并行实现方法。

* 每个线程负责计算结果矩阵C中的一个元素，如线程a负责计算矩阵C中的第i行，第j列的值，则该线程需要直接从global memory中访问矩阵A的第i列和矩阵B的第j列，对应计算差平方和后再开方。为简化该思路在实验中的表示，记其为“EulerDistance_1”。
* 注意到“EulerDistance_1”中，所有的线程都去访问global memory，其“compute-to-global-memory-access ratio”为1，由于访问global memory带宽的瓶颈的限制，并没有充分利用device的计算资源。所以类比于“矩阵乘矩阵”的思路，使用Tiling技术来减少对global memory的访问次数。记其为“EulerDistance_2”。
* 通过查阅资料，了解到调用“cudaMempy2D”复制二维数组有利于对齐，节省访问数据的时间，所以对原始的“cudaMempy”进行替换，记其为“EulerDistance_3”。

### 实现“EulerDistance_1”

这个实现非常直白，定义block为二维的，其能容纳线程数最好通过query device后赋值（还没实现），block中的每个线程负责计算结果矩阵中的一个元素，该kernel如下：

```c++
__global__
void EulerDistance_1(float* A,
                     float* B,
                     float* C,
                     int    m,
                     int    n,
                     int    dim){
    
    int bx = blockIdx.x;
    int by = blockIdx.y;
    int tx = threadIdx.x;
    int ty = threadIdx.y;
    
    int row = by * blockDim.y + ty;
    int col = bx * blockDim.x + tx;
    
    if(row < m && col < n){
        float dist = 0;
        for(int i = 0; i < dim;i++){
            float diff = A[i * m + row] - B[i * n + col];
            dist += diff * diff;
        }
        C[row * n + col] = sqrtf(dist);
    }
}
```

### 实现“EulerDistance_2”

这个实现受经典kernel“矩阵乘矩阵”的启发，使用Tiling技术，block中的每个线程先读取对应数据到shared memory中，以备block中的其他线程在计算时复用，从而减少访问global memory的次数，避免拥塞。

一开始我蜜汁自信的直接基于“矩阵乘矩阵”的kernel想当然的进行修改，调了大半天输出结果和串行计算结果总对不上，一定要注意的线程访问的下标和边界处理，始终坚持基于“每一个线程负责计算结果矩阵C中的一个元素”思考，不然很容易出错，该kernel实现如下：

```c++
__global__
void EulerDistance_2(float* A,
                     float* B,
                     float* C,
                     int    m,
                     int    n,
                     int    dim){
    
    __shared__ float Ads[TILE_WIDTH][TILE_WIDTH];
    __shared__ float Bds[TILE_WIDTH][TILE_WIDTH];
    
    int bx = blockIdx.x;
    int by = blockIdx.y;
    int tx = threadIdx.x;
    int ty = threadIdx.y;
    
    int row = by * blockDim.y + ty;
    int col = bx * blockDim.x + tx;
    
    int phase_num = ceil(dim / (float)TILE_WIDTH);
    
    int cond0 = row < m;
    int cond1 = col < n;
    
    float dist = 0.0;
    // 负责加载A的第row列和B的第col列
    for (int ph = 0; ph < phase_num;ph++) {
        if(ph * TILE_WIDTH + ty < dim){
            // 尤其这里很容易出错
            Ads[tx][ty] = (cond0) ? A[(ph * TILE_WIDTH + tx) * m + row] : 0;
            Bds[ty][tx] = (cond1) ? B[(ph * TILE_WIDTH + ty) * n + col] : 0;
        }
        else{
            Ads[tx][ty] = 0;
            Bds[ty][tx] = 0;
        }
        
        __syncthreads();
        
        if(cond1 &&cond0){
            for(int i = 0;i < TILE_WIDTH;i++){
                float diff = Ads[i][ty] - Bds[i][tx];
                dist += diff * diff;
            }
        }
        
        __syncthreads();
    }
    
    if(cond1 && cond0){
        C[row * n + col] = sqrtf(dist);
    }
}
```

### 实现“EulerDistance_3”

本来以为实现了上面“EulerDistance_2”就差不多，查资料发现，CUDA还提供了一组“cudaMallocPatch”和“cudaMemcpy2D”的方法，通过查阅官方文档和手册后，发现对于二维数组使用这组方法分配内存和传递数组元素较为高效，原因如下：

* 访问global memory的一个memory transaction，通常会一次性返回32或64或128个bytes，不论wrap中的线程是否需要这些额外的数据。要求一个memory transaction能访问的地址需是32/64/128的倍数。
* 根据上一条，如果使用“cudaMalloc”分配二维数组，二维数组被映射为一维数组，第二行的第一个元素被视为第一行最后一个元素的下一个元素，而矩阵的宽度在很多时候都不是32/64/128的倍数，这就导致，当访问某行的第一个元素时，其地址很大概率不是32/64/128的倍数。
* 为了解决这个问题，使用“cudaMallocPatch”方法可以使行对齐，这样，访问某行第一个元素其地址可以看做是0，这样就能更高效的访问每一行元素。

“EulerDistance_3”和“EulerDistance_2”的不同之处仅在于stub函数中分配device上数组的方式。

## 基于cublas的gemm实现

欧拉距离可以通过经典的“矩阵乘矩阵”问题转化求解来避免循环，而“矩阵乘矩阵”可以通过直接调用cublas中的gemm函数实现。具体转换方法如下：

设矩阵A为k\*m，矩阵B为k\*n，最后输出的距离矩阵为m\*n，定义Dij为矩阵A的第i列和矩阵B的第j列所代表向量间的欧拉距离。
$$
D_{ij} = (A_i - B_j)(A_i-B_j)^T
$$
其中Ai，Bj分表为A的第i列向量和B的第j列向量，展开上述公式：
$$
D_{ij} = A_i\times(A_i^T) + B_j\times(B_j^T) - A_i\times(B_j^T) - B_j\times(A_i^T)
$$
扩展到整个矩阵可得：
$$
D_{m\times n}= H_{m\times m} + K_{n\times n} - M_{m\times n} - N_{n\times m}
$$
而很明显由于不同秩问题无法计算，拿出单个元素来分析：

* 第一项影响结果D中的每一行，即一行中所有元素加的是同一个值。
* 第二项影响结果D中的每一列，即一列中所有元素加的是同一个值。
* 第三项和第四项互为转置。

其实最后可以将上式化为：
$$
D_{m\times n} = H_{m\times m} + K_{n\times n} - 2\times M_{m\times n}
$$
其中矩阵H中对角线上的元素需要进行行传播，矩阵K中的对角线上的元素需要进行列传播。

基于以上思路，将整个实现过程化为三步。

* 调用cublas的gemm函数求解矩阵M。
* 分别计算H和K中对角线元素，并存入一维数组中。
* 对上一步计算结果分别进行行/列扩展，并对最终的结果求开方。

记该实现算法为“EulerDistance_4”。

## 测试结果

测试过程应遵循

* 多次测试求平均。
* 较为重要的，在第一次调用kernel时会启动cuda环境，这时候计时会加入启动cuda环境的时间，这个应该避免。

### 设计测试数据

主要通过改变数据量的大小观察运行时间的变化并绘图。

```c++
int m = 1000,2000,4000,8000,16000;
int n = 1000,2000,4000,8000,16000;
int dim = 200,400,600,800,1000;
```

控制变量法做实验：

```
m = 1000, n = 1000, dim = 200
0：0.971021s
1：0.003095s
2：0.002289s
3：0.002315s
4：0.00255s
m = 2000, n = 1000, dim = 200
0：1.75639s
1：0.005452s
2：0.004053s
3：0.004165s
4：0.004311s
m = 4000, n = 1000, dim = 200
0：3.33481s
1：0.009443s
2：0.006776s
3：0.006815s
4：0.005919s
m = 8000, n = 1000, dim = 200
0：6.48812s
1：0.017842s
2：0.013116s
3：0.014058s
4：0.01072s
m = 16000, n = 1000, dim = 200
0：12.8131s
1：0.029783s
2：0.020771s
3：0.020959s
4：0.015079s

m = 1000, n = 2000, dim = 200
0：1.75691s
1：0.005719s
2：0.004199s
3：0.004251s
4：0.004075s
m = 1000, n = 4000, dim = 200
0：3.3348s
1：0.010392s
2：0.007074s
3：0.007099s
4：0.006206s
m = 1000, n = 8000, dim = 200
0：6.48509s
1：0.016459s
2：0.011136s
3：0.011169s
4：0.008298s
m = 1000, n = 16000, dim = 200
0：12.8144s
1：0.036465s
2：0.02493s
3：0.024742s
4：0.018237s

m = 1000, n = 1000, dim = 400
0：1.74179s
1：0.004831s
2：0.003304s
3：0.003353s
4：0.003096s
m = 1000, n = 1000, dim = 600
0：2.51449s
1：0.006433s
2：0.004126s
3：0.004221s
4：0.003854s
m = 1000, n = 1000, dim = 800
0：3.48057s
1：0.008344s
2：0.005097s
3：0.005057s
4：0.004182s
m = 1000, n = 1000, dim = 1000
0：4.06532s
1：0.009479s
2：0.007148s
3：0.006434s
4：0.004965s

m = 1000, n = 1000, dim = 8000
0：31.405s
1：0.060773s
2：0.040573s
3：0.040603s
4：0.021166s

m = 1000, n = 1000, dim = 15000
0：58.8057s
1：0.118681s
2：0.068036s
3：0.065984s
4：0.032738s
```

## 问题

* 有可能出现grid无法覆盖整个数据集。